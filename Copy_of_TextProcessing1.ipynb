{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paramjeetiitb/EE769_assignment1/blob/main/Copy_of_TextProcessing1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp_5zwmnUjym"
      },
      "source": [
        "# **Tutorial - 1**\n",
        "## **Text Pre-processing for Natural Language Processing (NLP)**\n",
        "\n",
        "Text preprocessing in NLP is the process of cleaning and preparing raw text data for analysis by machine learning or deep learning models. It involves converting text into a structured format through various steps. These steps ensure the text is consistent and meaningful, enabling models to better understand and process the data efficiently. Proper preprocessing is essential for improving the performance and accuracy of NLP models. The steps involved in pre-processing which we are going to be looking at in this tutorial are:\n",
        "1) **Lowercasing**\n",
        "2) **Removing Punctuations and Special Characrters**\n",
        "3) **Stop-Words removal**\n",
        "4) **Removal of URLs**\n",
        "5) **Removal of HTML Tags**\n",
        "6) **Stemming**\n",
        "7) **Lemmatization**\n",
        "8) **Tokenization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0nLXOyWUjyv"
      },
      "source": [
        "===================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHmxIOBaUjyw"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "\n",
        "## 1) **Lowercasing**\n",
        "Lowercasing converts all characters in a text to <blue>**lowercase**</blue>. It ensures uniformity by treating words like <blue>**\"Dog\"**</blue> and <blue>**\"dog\"**</blue> as the same entity. This is important for many NLP tasks since capitalization usually doesn't change the meaning of words.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Natural Language Processing\"\\\n",
        "Output: \"natural language processing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OoEvd3hNUjyx",
        "outputId": "c1016411-7624-49fa-e003-246ad95bdf20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " get a number of questions about this corpus each week, which i am unable to answer, mostly because they deal with preparation issues and such that i just don't know about. if you ask me a question and i don't answer, please don't feel slighted.\n"
          ]
        }
      ],
      "source": [
        "text = \" get a number of questions about this corpus each week, which I am unable to answer, mostly because they deal with preparation issues and such that I just don't know about. If you ask me a question and I don't answer, please don't feel slighted.\"\n",
        "lowercased_text = text.lower()\n",
        "\n",
        "print(lowercased_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3sjSXw-Ujy2"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 2) **Removing Punctuation & Special Characters**\n",
        "\n",
        "Punctuation marks (like <blue>**commas**</blue>, <blue>**periods**</blue>, <blue>**dash**</blue> etc.) and special characters (like <blue>**@**</blue>, <blue>**#**</blue>, <blue>**$**</blue>, etc.) are often not meaningful in many NLP tasks. Removing them helps clean the text for better analysis.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Hello! How are you doing @today?\"\\\n",
        "Output: \"Hello How are you doing today\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "liHdbCFAUjy3",
        "outputId": "b46dafb8-e3ae-480e-b89b-bc694425b487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " get a number of questions about this corpus each week which I am unable to answer mostly because they deal with preparation issues and such that I just dont know about If you ask me a question and I dont answer please dont feel slighted\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \" get a number of questions about this corpus each week, which I am unable to answer, mostly because they deal with preparation issues and such that I just don't know about. If you ask me a question and I don't answer, please don't feel slighted.\"\n",
        "punctuation_pattern = r'[^\\w\\s]'\n",
        "text_cleaned = re.sub(punctuation_pattern, '', text)\n",
        "print(text_cleaned)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ZMlV_aUjy5"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 3) **Stop - Words Removal**\n",
        "\n",
        "Stop-words are common words like <blue>**\"the\"**</blue>, <blue>**\"is\"**</blue>, <blue>**\"in\"**</blue>, <blue>**\"and\"**</blue> that don't contribute significant meaning to the text. Removing them helps reduce the size of the dataset <blue>**without losing important context**</blue>.\n",
        "\n",
        "Example:\\\n",
        "Input: \"This is a sample sentence\"\\\n",
        "Output: \"sample sentence\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LpbxPUlRUjy6",
        "outputId": "ed4240c7-88bd-49df-eef7-232abf41fc73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language: english\n",
            "Filtered Text: ['get', 'number', 'questions', 'corpus', 'week,', 'I', 'unable', 'answer,', 'mostly', 'deal', 'preparation', 'issues', 'I', 'know', 'about.', 'If', 'ask', 'question', 'I', 'answer,', 'please', 'feel', 'slighted.']\n",
            "Language: hinglish\n",
            "Filtered Text: ['Yeh', 'din', 'I', 'feeling', 'awesome']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Remove stopwords function for any language\n",
        "def remove_stopwords(text, language):\n",
        "    stop_words = set(stopwords.words(language))\n",
        "    word_tokens = text.split()\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    print(f\"Language: {language}\")\n",
        "    print(\"Filtered Text:\", filtered_text)\n",
        "\n",
        "# English Example\n",
        "en_text = \" get a number of questions about this corpus each week, which I am unable to answer, mostly because they deal with preparation issues and such that I just don't know about. If you ask me a question and I don't answer, please don't feel slighted.\"\n",
        "remove_stopwords(en_text, \"english\")\n",
        "\n",
        "# Hindi + English - Example\n",
        "hi_text = \"Yeh ek bahut accha din hai and I am feeling awesome\"\n",
        "remove_stopwords(hi_text, \"hinglish\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHBhqss2Ujy7"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 4) **Removal of URLs**\n",
        "\n",
        "URLs are often <blue>**irrelevant**</blue> in NLP tasks and can add noise to the data. Removing them ensures cleaner text without <blue>**web links**</blue> that don’t contribute to the context.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Check out this link: https://example.com\"\\\n",
        "Output: \"Check out this link\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lNowDd9-Ujy8",
        "outputId": "2aa2e90a-0f6d-4fe6-9790-4ec06519cf3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have taken the database for the corpus from '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "text = \"I have taken the database for the corpus from https://www.cs.cmu.edu/~./enron/\"\n",
        "remove_urls(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtZ5C35XUjy9"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 5) **Removal of HTML Tags**\n",
        "\n",
        "HTML tags are used in web data but are <blue>**unnecessary**</blue> in NLP tasks. <blue>**Stripping**</blue> out HTML tags cleans the text extracted from <blue>**web pages**</blue>.\n",
        "\n",
        "Example:\\\n",
        "Input: \"&lt;p>This is a paragraph.&lt;/p>\"\\\n",
        "Output: \"This is a paragraph.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LOtCWxsKUjy-",
        "outputId": "ccc69d99-3592-4d85-a10f-a44ac148626f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLP - Deep Learning\n",
            "Removal of HTML tags\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"<html><div>\n",
        "<h1>NLP - Deep Learning</h1>\n",
        "<p>Removal of HTML tags</p>\n",
        "<a href=\"https://example.com\"></a>\n",
        "</div></html>\"\"\"\n",
        "\n",
        "html_tags_pattern = r'<.*?>'\n",
        "\n",
        "text_without_html_tags = re.sub(html_tags_pattern, '', text)\n",
        "print(text_without_html_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7cih9qbUjy_"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 6) **Stemming**\n",
        "\n",
        "Stemming reduces a word to its <blue>**base**</blue> or <blue>**root**</blue> form, which might not always be a valid word. The idea is to <blue>**strip**</blue> off <blue>**prefixes**</blue> or <blue>**suffixes**</blue>. It’s a quick and less computationally expensive way of normalizing words. Stemming is preferred when the <blue>**meaning**</blue> of the word is <blue>**not important**</blue> for analysis. for example: <blue>**Spam Detection**</blue>\n",
        "\n",
        "Example:\\\n",
        "Input: \"Playing\", \"Played\", \"Plays\"\\\n",
        "Output: \"Play\"\n",
        "\n",
        "<blue>**Porter stemming**</blue> algorithm is one of the most common stemming algorithms which is basically designed to <blue>**remove**</blue> and <blue>**replace**</blue> well-known <blue>**suffixes**</blue> of English words. Although the Porter Stemming Algorithm was developed for English texts, it can be adapted to different languages. However, it is more effective to use natural language processing tools and algorithms specifically designed for different languages, like the library <blue>**iNLTK**</blue> offers these tools for <blue>**Indic Languages**</blue>. You can find it out here: <blue>**https://github.com/goru001/inltk**</blue>\n",
        "\n",
        "<div style=\"font-style: italic; text-align: center;\" markdown=\"1\">\n",
        "<img width=\"30%\" src=\"https://cdn.botpenguin.com/assets/website/Stemming_53678d43bc.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pa129c9JUjy_",
        "outputId": "247fbb54-e7ff-4b6f-fe6b-9a3cd3b28b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'think', 'the', 'better', 'way', 'will', 'also', 'be', 'to', 'keep', 'learn', 'algorithm', 'while', 'run', 'code']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    word_tokens = text.split()\n",
        "    stems = [stemmer.stem(word) for word in word_tokens]\n",
        "    return stems\n",
        "\n",
        "text = ' I think the better way will also be to keep learning algorithm while running code'\n",
        "print(stem_words(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acSrnVfCUjzA"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 7) **Lemmatization**\n",
        "\n",
        "Lemmatization is a more advanced technique compared to stemming. It <blue>**reduces**</blue> a word to its <blue>**base form (called a lemma)**</blue> while ensuring the <blue>**output**</blue> is a <blue>**valid word**</blue>. It uses context to determine whether the word is in singular, plural, or tense forms.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Running\", \"Ran\"\\\n",
        "Output: \"Run\"\n",
        "\n",
        "In our lemmatization example, we will be using a popular lemmatizer called <blue>**WordNet**</blue> lemmatizer. WordNet is a word association database for English and a useful resource for English lemmatization. A popular lemmatizer used for Hindi is developed by <blue>**JohSnowLabs**</blue> can be found here: <blue>**https://sparknlp.org/2020/07/29/lemma_hi.html**</blue>\n",
        "\n",
        "<div style=\"font-style: italic; text-align: center;\" markdown=\"1\">\n",
        "<img width=\"30%\" src=\"https://cdn.botpenguin.com/assets/website/Lemmatization_5338fc7c3e.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AyAwyFifUjzA",
        "outputId": "cae75154-8d58-4a38-c767-420879d4a1d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'be', 'no', 'other', 'way', 'other', 'than', 'to', 'accept', 'change', 'and', 'to', 'keep', 'change', 'oneself', 'to', 'keep', 'one', 'abreast', 'of', 'time']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_word(text):\n",
        "    word_tokens = text.split()\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
        "    return lemmas\n",
        "\n",
        "text = 'There is no other way other than to accept change and to keep changing oneself to keep one abreast of time'\n",
        "print(lemmatize_word(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlnUXNtPUjzB"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "</style>\n",
        "## 8) **Tokenization**\n",
        "\n",
        "Tokenization is the process of <blue>**splitting**</blue> a text into <blue>**individual units**</blue> like words, phrases, or sentences, called <blue>**tokens**</blue>. These tokens form the building blocks for further processing and analysis in NLP tasks.\n",
        "\n",
        "Example:\\\n",
        "Input: \"Congratulations you are almost at the end of this file.\"\\\n",
        "Output: [\"Congratulations\", \"you\", \"are\", \"almost\", \"at\", \"the\", \"end\", \"of\", \"this\", \"file\", \".\"]\n",
        "\n",
        "There are different methods and libraries available to perform tokenization. <blue>**SpaCy**</blue> and <blue>**Gensim**</blue> are some of the libraries that can be used to accomplish the task.\n",
        "Tokenization can be used to separate words or sentences. If the text is split into <blue>**words**</blue> using some separation technique it is called <blue>**word tokenization**</blue> and the same separation done for <blue>**sentences**</blue> is called <blue>**sentence tokenization**</blue>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8Wn0QOr3UjzB",
        "outputId": "c02000ea-4312-4724-c0ae-af93f37db227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'think', 'that', 'this', 'assignment', 'is', 'a', 'very', 'good', 'start', 'to', 'this', 'course', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"I think that this assignment is a very good start to this course.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnkj2w5LUjzB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}